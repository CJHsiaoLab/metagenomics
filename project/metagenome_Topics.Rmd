---
title: "Topic model for metagenomic data"
author: "Kushal K Dey"
date: "December 29, 2015"
output: pdf_document
---

## Introduction

Topic model or admixture type models can be used for clustering metagenomic samples in 16s RNA counts data. However, these topic models while assuming all the features (the Operational Taxonomic Units or OTUs) to be independent. However, several OTUs may belong to the same species, and again several species may form one family of the microbiome. This hierarchical structure in the features is something we may be interested in exploring going forward and modifying the topic model accordingly. I present here the model which may be used to fit the topic model taking into account this hierarchical structure.

The core idea behind this model has been derived from the Multiscale Topic Tomography model described in this [paper](http://www.cs.cmu.edu/~wcohen/postscript/topic-tomography-submitted.pdf).

## The Model

Let us start with the counts data $c_{N \times G}$ where $N$ represents the number of samples and $G$ represents the number of OTUs. Using Matt Taddy's model, we can write 

$$ c_{n*} | c_{n.} \sim Mult ( c_{n.}, p_{n*}) $$

$$ p_{ng} = \sum_{k=1}^{K} \omega_{nk} \theta_{kg} $$

#### Multi-resolution model for topics 

We build the hierarchical tree as follows. Let there be $S$ levels in the hierarchical tree. The OTUs form the leaves of the tree, while the other levels may represent the family, species, genus etc.


\begin{align}
\theta^{(S)}_{kl} & = \theta_{kl} \hspace{1 cm} l=0,1,2,\cdots, N_{S}-1  \\
\theta^{(s)}_{kl} & = \sum_{h: i_{s}(h) = l} \theta^{(s+1)}_{kh} \hspace{1 cm} s=0,1,2,\cdots, S-1, \hspace{1 cm} l=0,1,2,\cdots, N_{s}-1 \\
\end{align}

where $N_{s}$ represents the number of leaves if the tree is truncated at level $s$ and $i_{s}(.)$ is a function that takes the unit in level $s+1$ and maps it to the family it belongs to in level $s$. Note that $N_{S}=G$.

#### Latent representation of model


Now if we assume that 

$$ c_{n.} \sim Poi(\lambda_{n}) $$

Then one can write 

$$ c_{nl} \sim Poi (\lambda_{n} \sum_{k=1}^{K} \omega_{nk} \theta^{(S)}_{kl}) $$

Let $z_{nkg}$ represents the number of counts from sample $n$ and from OTU $l$ that comes from $k$ th subgroup or cluster. By definition,

$$ \sum_{k=1}^{K} z_{nkl} = c_{nl}  $$

Since the summation of two independent Poisson random variables is also a Poisson variable with mean equal to the sum of the means of the
original random variables, we can infer that

$$ z_{nkl} \sim Poi(\lambda_{n}\omega_{nk} \theta^{(S)}_{kl}) $$

Let $z_{kg}$ represents the number of latent counts coming from the $k$ th subgroup and feature $g$ across all the samples.

$$ z_{kl} = \sum_{n=1}^{N} z_{nkl} $$

So, 

$$ z_{kl} \sim Poi(\theta^{(S)}_{kl} \sum_{n=1}^{N} \lambda_{n} \omega_{nk}) $$

#### Multi-resolution model for latent variables


Suppose we are at a particular iterative step of our model where we have plausible values of $\omega$ and $\theta$ (we can start with the same prior for these parameters as Taddy model). Given $\omega$, we use the following step to estimate a refined $\theta$.

From Eqn 8 of Matt Taddy's [paper](http://arxiv.org/pdf/1109.4518v3.pdf)), we can write

$$ z_{nkl} = c_{nl} \frac{\omega_{nk} \theta_{kl}}{\sum_{h=1}^{K} \omega_{nh} \theta_{hl}} $$

So, 

$$ z_{kl} = \sum_{n=1}^{N} c_{ng} \frac{\omega_{nk} \theta_{kl}}{\sum_{h=1}^{K} \omega_{nh} \theta_{hl}} $$

Note that $z_{kg}$ and $z_{k^{'}g}$ for $k \neq k^{'}$ are independent. Then the multiscale framework for $\theta$ can be translated to multiscale framework for $z$ as well. Under this framework, we have 

\begin{align}
z^{(S)}_{kl} & = z_{kl} \hspace{1 cm} l=0,1,2,\cdots, N_{S}-1 \\
z^{(s)}_{k(l)} & =  \sum_{h: i_{s}(h) = l} z^{(s+1)}_{kh} \hspace{1 cm} s=0,1,2,\cdots, S-1, \hspace{1 cm} l=0,1,2,\cdots, N_{s} -1 \\
\end{align}

We now define 

$$ \mu^{(s)}_{kl} = \sum_{n=1}^{N} \lambda_{n} \omega_{nk} \theta^{(s)}_{kl} \hspace{1 cm} l=0,1,2,\cdots, N_{s} -1 $$

and it can be shown easily that 

$$ z^{(s)}_{k(l)} \sim Poi(\mu^{(s)}_{kl}) \hspace{1 cm} l=0,1,2,\cdots, N_{s} -1 $$


#### Transformation of variables on hierarchy

Instead of using $\mu^{(s)}_{kg}$ along the multi-resolution tree, we transform the parameters as follows 

$$ \beta^{(s)}_{k(l,h_{l})} = \frac{\mu^{(s+1)}_{kh_{l}}}{\mu^{(s)}_{kl}} \hspace{0.5 cm} s=0,1,2,\cdots, S-1, \hspace{0.5 cm} l=0,1,2,\cdots, N_{s}-1, \hspace{0.4 cm} i_{s}(h_{l})=l,  \hspace{0.4 cm} l=1,2,\cdots, c(l) $$

where $c(l)$ is the number of children of the node $l$. 

We only need the highest level wavelet parameter $\mu^{(0)}_{k0}$ and $\beta^{(s)}_{k(l,h_{l})}$ instead of $\mu^{(s)}_{kl}$. We work on these transformed parameter space. The transformed parameters are easy to work with as they are independent. We assume the priors to be 

$$ \mu^{(0)}_{k0} \sim Gamma(.| \nu_{\mu}, \delta_{\mu}) $$

$$ \beta^{(s)}_{k(l, .)} \sim Dir_{c(l)} \left (. | \frac{1}{c(l)}, \frac{1}{c(l)}, \cdots, \frac{1}{c(l)} \right ) $$

where $c(l)$ is the number of children for the node $l$. 

#### Prior on wavelet parameters


The prior distribution is therefore given by 

$$ P(\mu | \delta) = \prod_{k=1}^{K} Gamma( \mu^{(0)}_{k0} | \nu_{\mu}, \delta_{\mu}) \times \prod_{k=1}^{K} \prod_{s=0}^{S-1} \prod_{l=0}^{N_{s}-1} Dir_{c(l)} \left (\beta^{(s)}_{k(l, .)} | \frac{1}{c(l)}, \frac{1}{c(l)}, \cdots, \frac{1}{c(l)} \right ) $$

#### Loglikelihood given wavelet parameters


The loglikelihood of $\mu$ is given as follows 

\begin{align}
L(\mu)  & = \sum_{l=0}^{2^{S}-1} \sum_{k=1}^{K} log Poi (z^{(S)}_{k(l)} | \mu^{(S)}_{kl}) \\ 
& = \sum_{s=0}^{S-1} \sum_{l=0}^{N_{s}-1} \sum _{k=1}^{K} log Mult \left ( z^{(s+1)}_{k,.} | \beta^{(s)}_{k(l,1)}, \beta^{(s)}_{k(l,2)}, \cdots, \beta^{(s)}_{k(l,c(l))} \right )
+ \sum_{k=1}^{K} log Poi (z^{(0)}_{k(0)} | \mu^{(0)}_{k0}) \\
\end{align}

The $z$ values estimated may not always be integers but we assume that they are approximated to the nearest integer. This is the same policy also adopted by the authors in the multiscale Topic Tomography [paper](http://www.cs.cmu.edu/~wcohen/postscript/topic-tomography-submitted.pdf). 

#### MAP estimates of wavelet parameters

Given the prior and the log likelihood functions reported above, one can compute th log posterior of the $\mu$ and then one can update the parameters using their MAP estimates.

$$ \beta^{(s)}_{k(l,h_{l})} = \frac{z^{(s+1)}_{kh_{l}} + \delta_{\beta}-1}{z^{(s)}_{kl} + 2(\delta_{\beta}-1)} \hspace{1 cm} h_{l}=1,2,\cdots, c(l) \hspace{1 cm} \forall k$$

$$ \mu^{(0)}_{k0} = \frac{z^{(0)}_{k(0)} + \nu_{\mu}-1}{\delta_{\mu}+1} \hspace{1 cm} \forall k$$


This helps us generate the $\mu^{(s)}_{kl}$ for all $s,k,l$ and most importantly $\mu^{(S)}_{kl}$. Given that we know $\mu^{(S)}_{kl}$, we can compute the variables of interest $\theta$ as 

$$ \theta^{(S)}_{kl} = \frac{\mu^{(S)}_{kl}} { \sum_{r=1}^{G} \mu^{(S)}_{kr}} $$

#### Updating topic proportions 

These are the $\theta$ update of the step. The $\theta^{(S)}$ values updated this way can then be used to update the $\omega$ parameters, which incidentally depend only on the leaf node parameters $\theta^{(S)}$. The approach to estimating $\omega$ is similar to the one used by  Matt Taddy, using active set strategy. 



